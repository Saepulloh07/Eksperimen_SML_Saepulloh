name: Wine Quality Preprocessing Pipeline

on:
  push:
    branches: [main, develop]
    paths:
      - "preprosessing/**"
      - ".github/workflows/preprocessing-pipeline.yml"
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      dataset_url:
        description: "Dataset URL"
        required: false
        default: "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"

jobs:
  preprocessing:
    name: Data Preprocessing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"
          cache: pip

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas numpy scikit-learn matplotlib seaborn joblib

      - name: Verify Preprocessing Folder Exists
        run: |
          echo "Current directory structure:"
          ls -la
          echo "Checking preprosessing folder..."
          ls -la preprosessing/ || (echo "Folder preprosessing not found!" && exit 1)

      - name: Run Preprocessing Pipeline
        working-directory: ./preprosessing
        run: |
          echo "Starting preprocessing pipeline..."
          python automation_Saepulloh.py
          echo "Preprocessing completed."

      - name: Verify Output Files
        working-directory: ./preprosessing/data/output
        run: |
          echo "Generated files:"
          ls -lh

          required_files=(
            "X_train_scaled.npy"
            "X_test_scaled.npy"
            "y_train.npy"
            "y_test.npy"
            "scaler.pkl"
            "feature_names.pkl"
          )
          for file in "${required_files[@]}"; do
            [ -f "$file" ] && echo "$file exists" || (echo "$file missing" && exit 1)
          done

      - name: Generate Dataset Statistics
        working-directory: ./preprosessing
        run: |
          python - <<'PY'
          import numpy as np
          import pickle

          print("\nDataset Statistics")
          print("=" * 50)
          X_train = np.load('data/output/X_train_scaled.npy')
          X_test  = np.load('data/output/X_test_scaled.npy')
          y_train = np.load('data/output/y_train.npy')
          y_test  = np.load('data/output/y_test.npy')

          with open('data/output/feature_names.pkl', 'rb') as f:
              features = pickle.load(f)

          print(f"Training samples   : {X_train.shape[0]}")
          print(f"Test samples       : {X_test.shape[0]}")
          print(f"Number of features : {len(features)}")
          print(f"Train labels       : {dict(zip(*np.unique(y_train, return_counts=True)))}")
          print(f"Test labels        : {dict(zip(*np.unique(y_test, return_counts=True)))}")
          print("=" * 50)
          PY

      - name: Upload Preprocessed Data
        uses: actions/upload-artifact@v4
        with:
          name: preprocessed-wine-data
          path: preprosessing/data/output/
          retention-days: 30

      - name: Create Summary Report
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ## Preprocessing Pipeline Summary

          Status: SUCCESS

          Generated files:
          - X_train_scaled.npy
          - X_test_scaled.npy
          - y_train.npy
          - y_test.npy
          - scaler.pkl
          - feature_names.pkl

          Next: Download the artifact `preprocessed-wine-data` and continue with model training.
          EOF

      - name: Comment on Pull Request
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'Preprocessing pipeline finished successfully!\nPreprocessed data is available as artifact `preprocessed-wine-data`.'
            })

  data-validation:
    name: Data Quality Checks
    runs-on: ubuntu-latest
    needs: preprocessing

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Download Artifact
        uses: actions/download-artifact@v4
        with:
          name: preprocessed-wine-data
          path: ./artifacts/

      - name: Validate Data Quality
        run: |
          python - <<'PY'
          import numpy as np
          import sys, os

          print("Data Quality Checks")
          print("="*50)

          X_train = np.load('artifacts/X_train_scaled.npy')
          X_test  = np.load('artifacts/X_test_scaled.npy')
          y_train = np.load('artifacts/y_train.npy')
          y_test  = np.load('artifacts/y_test.npy')

          errors = []

          if np.isnan(X_train).any() or np.isnan(X_test).any():
              errors.append("NaN values found")

          mean = X_train.mean()
          std  = X_train.std()
          if not (-0.1 < mean < 0.1 and 0.9 < std < 1.1):
              errors.append(f"Scaling suspicious â€“ mean={mean:.4f}, std={std:.4f}")

          if set(np.unique(y_train)) != {0, 1} or set(np.unique(y_test)) != {0, 1}:
              errors.append("Labels are not binary (0/1)")

          ratio = len(y_train) / (len(y_train) + len(y_test))
          if not (0.7 <= ratio <= 0.9):
              errors.append(f"Train/test split unusual: {ratio:.2%}")

          print("="*50)
          if errors:
              print("Issues found:")
              for e in errors: print(" -", e)
              sys.exit(1)
          else:
              print("All quality checks passed")
          PY
